-#
  # $HeadURL$
  # $Id$
  #
  # Copyright (c) 2009-2010 by Public Library of Science, a non-profit corporation
  # http://www.plos.org/
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
- content_for :head do
  = javascript_include_tag 'jquery.ba-hashchange.min'
  :javascript
    $(function() {
      $('#content section').attr('id', function(i, id) { return '_' + id; });
      $(window).bind('hashchange', function() {
        var id =
          $('#content section')
            .hide()
            .filter(location.hash.replace('#', '#_') || ':first')
              .show()
              .attr('id');
        $('#content nav a')
          .removeClass('current')
          .filter('[href=#' + id.replace('_', '') + ']')
            .addClass('current');
      });
      $(window).trigger('hashchange');
    });

%nav
  %ul
    %li
      %a{ :href => "#research" } Research in context
    |
    %li
      %a{ :href => "#interpreting" } Interpreting the data
    |
    %li
      %a{ :href => "#analysis" } Data analysis and evaluation
    |
    %li
      %a{ :href => "#videos" } Videos

.border
  .page
    %section#research
      :markdown
        Research in context
        ===================
  
        We want to place transparent and comprehensive information about the usage
        and reach of published articles onto the articles themselves, so that the
        entire academic community can assess their value. We call these measures
        for evaluating articles 'Article-Level Metrics', and they are distinct from
        the journal-level measures of research quality that have traditionally been
        made available until now.
  
        So far the available data includes...
  
          * Article usage statistics - HTML pageviews, PDF downloads  and XML downloads
          * Citations from the scholarly literature – currently from [PubMed Central], [Scopus] and [CrossRef]
          * Social bookmarks - currently from [CiteULike] and [Connotea]
          * Comments – left by readers of each article
          * Notes – left by readers of each article
          * Blog posts – aggregated from [Postgenomic], [Nature Blogs], [Bloglines] and [ResearchBlogging].
          * Ratings – left by readers of each article
  
        ... and we’re not finished yet. We're working to develop further measures
        and to refine and develop tools that allow users to  search and sort
        articles on the basis of these metrics. We're continuing to expand the
        Article-Level Metrics program because we believe that articles should be
        considered on their own merits, and that the impact of an individual
        article should not be determined by the journal in which it happened to be
        published.  As a result, we hope that new ways of measuring and evaluating
        research quality (or 'impact') can and will evolve.
  
        [PubMed Central]:http://www.pubmedcentral.nih.gov/
        [Scopus]:http://www.scopus.com/home.url
        [CrossRef]:http://www.crossref.org/
        [CiteULike]:http://www.citeulike.org/
        [Connotea]:http://www.connotea.org/
        [Postgenomic]:http://www.postgenomic.com/
        [Nature Blogs]:http://blogs.nature.com/
        [Bloglines]:http://www.bloglines.com/
        [ResearchBlogging]:http://www.researchblogging.org/
  
    %section#interpreting
      :markdown
        Interpreting the data
        =====================
  
        Because they are so new, Article-Level Metrics (and usage data in
        particular) should be interpreted with care. Therefore, when  looking at
        Article-Level Metrics for the first time bear the following points in mind:
  
          * Online usage is dependent on the article type, the age of the article, and the subject area(s) it is in. Therefore you should be aware of these effects when considering the performance of any given article.
          * Older articles normally have higher usage than younger ones simply because the usage  has had longer to accumulate. Articles typically have a peak in their usage in the first 3 months and usage then levels off after that.
          * Spikes of usage can be caused by media coverage, usage by large numbers of people, out of control download scripts or any number of other reasons. Without a detailed look at the raw usage logs it is often impossible to tell what the reason is and so we encourage you to regard usage data as indicative of trends, rather than as an absolute measure for any given article.
          * Newly published articles do not accumulate usage data instantaneously but require a day or two before data are shown.
          * Article citations as recorded by the Scopus database are sometimes undercounted because there are two records in the database for the same article. We’re working with Scopus to correct this issue.
          * Commenting, Notes, Rating, and Trackback functionalities were made live on each of our journals at different times. Articles published before the functionality went live will show lower figures.
          * All metrics will accrue over time (and some, such as citations, will take several years to accrue). Therefore, recent articles may not show many metrics (other than online usage, which accrues from day one).
  
        To help you see where an article of interest sits compared to certain
        average measures, we are providing journal summary tables as well as
        providing the entire data set as a download.
  
    %section#analysis
      :markdown
        Data analysis and evaluation
        ============================
  
        External analysis – since launching Article-Level Metrics in March 2009, a
        number of different groups and individuals have independently analyzed
        different aspects of the dataset. Here are some projects that caught our
        attention:
  
          * [The Spread of Scientific Information: insights from the web access statistics in PLoS Article-level Metrics Dataset](http://homes.gersteinlab.org/people/kkyan/Official_Page/plos_ana.html)<br/>
            Koon-Kiu Yan and Mark Gerstein, Department of Molecular Biophysics and
            Biochemistry, Yale University, U.S.A.  This study empirically examines
            how papers published online gain popularity in the community and the
            rate of spread of information.
          * [Mutable Mobiles: Online Journals and the Evolving Genre Ecosystem of Science](http://www.lib.ncsu.edu/theses/available/etd-07222009-144636/unrestricted/etd.pdf)<br/>
            Christian Casper, Department of Communication, Rhetoric and Digital
            Media, North Carolina State University, USA.  This dissertation uses
            both quantitative and qualitative/critical methods to analyze possible
            new interactions between online texts that differ from those of print
            texts.
          * [Analysis: Correlating the PLoS Article-level metrics](http://larsjuhljensen.wordpress.com/2010/01/15/analysis-correlating-the-plos-article-level-metrics/)<br/>
            Lars Juhl Jensen, Novo Nordisk Foundation Center for Protein Research,
            Copenhagen.  This blog post analyses how various categories of metrics
            (such as bookmarks, blog posts, citations, downloads, ratings and
            trackbacks) correlate with each other.  Interestingly, the author finds
            that after downloads, bookmarks have the next highest correlation with
            citations.
          * [Article-Level Metrics on FriendFeed](http://friendfeed.com/article-level-metrics)<br/>
            An open discussion forum where the community collates and discusses
            various article-level metrics developments.
  
        Checking the data – if you spot some missing data or something that just
        looks plain weird, please let us know by sending a note to the PLoS team,
        [alm@plos.org](mailto:alm@plos.org).
        
    %section#videos
      :markdown
        Videos
        ======

        Various videos about Article-Level Metrics have been created, some by the community and some by us. Enjoy.
        
        1. [How Article-Level Metrics can make your life easier](http://vimeo.com/5696434) (focuses on social bookmarking) – by Cameron Neylon, Biological Scientist, [Academic Editor on PLoS ONE](http://www.plosone.org/static/edboard.action), runs the blog [Science in the Open](http://blog.openwetware.org/scienceintheopen/), and works in the UK at the Science and Technology Facilities Council, **7 minutes**
        
        2. [A walk through of Article-Level Metrics](http://www.youtube.com/watch?v=jHUqwxIxgZQ&feature=autoshare_twitter) on a PLoS Pathogens article – by Alan Cann, Department of Biology at the University of Leicester, UK who also writes the blog [Science of the Invisible](http://scienceoftheinvisible.blogspot.com/), **2 minutes**
        
        3. [The blog aggregrators that appear in Article-Level Metrics](http://screenr.com/H8U) (focuses on ResearchBlogging.org) – by Liz Allen, Director of MarCom, PLoS, **2.5 minutes**
        
        4. [A comprehensive presentation of Article-Level Metrics at UC Berkeley](http://www.youtube.com/watch?v=Z05j5fsVfHA) – by Pete Binfield, Publisher (PLoS ONE and the Community Journals), **1hr**
        
        5. [Academic Publishing in Europe – presentation of Article-Level Metrics at the 5th international conference](http://river-valley.tv/media/conferences/ape2010/0203-Mark-Patterson/) – by Mark Patterson (Publishing Director, PLoS), **30 mins**
